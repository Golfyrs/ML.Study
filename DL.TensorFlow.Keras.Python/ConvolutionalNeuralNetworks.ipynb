{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bit482021fea1c847e3a486dbbd075e16c7",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import  Sequential\n",
    "from tensorflow.keras.layers import  Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "import pickle\n",
    "from numpy import array\n",
    "\n",
    "pickle_in = open(\"x_train.pickle\", \"rb\")\n",
    "x_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"x_test.pickle\", \"rb\")\n",
    "x_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y_train.pickle\", \"rb\")\n",
    "y_train = array(pickle.load(pickle_in))\n",
    "\n",
    "pickle_in = open(\"y_test.pickle\", \"rb\")\n",
    "y_test = array(pickle.load(pickle_in))\n",
    "\n",
    "# Normalize\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "#x_train = tensorflow.keras.utils.normalize(x_train, axis = 1) # try to use TS.normalize\n",
    "#x_test = tensorflow.keras.utils.normalize(x_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add( Conv2D(256, (3,3), input_shape = x_train.shape[1:]) )\n",
    "model.add( Activation('relu') )\n",
    "model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model.add( Conv2D(256, (3,3) ))\n",
    "model.add( Activation('relu') )\n",
    "model.add( MaxPooling2D(pool_size=(2,2)) )\n",
    "\n",
    "model.add( Flatten() )\n",
    "model.add( Dense(64) )\n",
    "\n",
    "model.add( Dense(1) )\n",
    "model.add( Activation('sigmoid') )\n",
    "\n",
    "#optimizer='rmsprop',\n",
    "#loss='categorical_crossentropy',\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.ndim)\n",
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN_last_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(\"_________________________\")\n",
    "    print(\"Epochs: \", i)\n",
    "    model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2)\n",
    "    model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Num GPUs Available:  0\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>loss='binary_crossentropy',\n",
    "<br>optimizer='adam',\n",
    "<br>metrics=['accuracy']\n",
    "\n",
    "<br>_________________________\n",
    "<br>Epochs:  0\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 209s 13ms/sample - loss: 0.6471 - accuracy: 0.6228 - val_loss: 0.6040 - val_accuracy: 0.6746\n",
    "<br>4990/4990 - 16s - loss: 0.6145 - accuracy: 0.6635\n",
    "<br>_________________________\n",
    "<br>Epochs:  1\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 212s 13ms/sample - loss: 0.5795 - accuracy: 0.7024 - val_loss: 0.5836 - val_accuracy: 0.6811\n",
    "<br>4990/4990 - 17s - loss: 0.5974 - accuracy: 0.6820\n",
    "<br>_________________________\n",
    "<br>Epochs:  2\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 201s 13ms/sample - loss: 0.5426 - accuracy: 0.7286 - val_loss: 0.5092 - val_accuracy: 0.7553\n",
    "<br>4990/4990 - 16s - loss: 0.5253 - accuracy: 0.7453\n",
    "<br>_________________________\n",
    "<br>Epochs:  3\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 194s 12ms/sample - loss: 0.5002 - accuracy: 0.7595 - val_loss: 0.4830 - val_accuracy: 0.7683\n",
    "<br>4990/4990 - 15s - loss: 0.5051 - accuracy: 0.7583\n",
    "<br>_________________________\n",
    "<br>Epochs:  4\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 199s 12ms/sample - loss: 0.4688 - accuracy: 0.7785 - val_loss: 0.4866 - val_accuracy: 0.7690\n",
    "<br>4990/4990 - 16s - loss: 0.5007 - accuracy: 0.7591\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>loss='binary_crossentropy',\n",
    "<br>optimizer='rmsprop'\n",
    "\n",
    "<br>Epochs:  0\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 202s 13ms/sample - loss: 0.6961 - accuracy: 0.6394 - val_loss: 0.5373 - val_accuracy: 0.7267\n",
    "<br>4990/4990 - 15s - loss: 0.5566 - accuracy: 0.7134\n",
    "<br>_________________________\n",
    "<br>Epochs:  1\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 200s 13ms/sample - loss: 0.5386 - accuracy: 0.7380 - val_loss: 0.6683 - val_accuracy: 0.6759\n",
    "<br>4990/4990 - 16s - loss: 0.6757 - accuracy: 0.6699\n",
    "<br>_________________________\n",
    "<br>Epochs:  2\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 199s 12ms/sample - loss: 0.4770 - accuracy: 0.7766 - val_loss: 0.4576 - val_accuracy: 0.7881\n",
    "<br>4990/4990 - 16s - loss: 0.4696 - accuracy: 0.7780\n",
    "<br>_________________________\n",
    "<br>Epochs:  3\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 201s 13ms/sample - loss: 0.4331 - accuracy: 0.8017 - val_loss: 0.4903 - val_accuracy: 0.7756\n",
    "<br>4990/4990 - 16s - loss: 0.4946 - accuracy: 0.7729\n",
    "<br>_________________________\n",
    "<br>Epochs:  4\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 201s 13ms/sample - loss: 0.3985 - accuracy: 0.8185 - val_loss: 0.4993 - val_accuracy: 0.7758\n",
    "<br>4990/4990 - 16s - loss: 0.5055 - accuracy: 0.7667\n",
    "<br>_________________________\n",
    "<br>Epochs:  5\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 202s 13ms/sample - loss: 0.3659 - accuracy: 0.8369 - val_loss: 0.4430 - val_accuracy: 0.7946\n",
    "<br>4990/4990 - 15s - loss: 0.4468 - accuracy: 0.7940\n",
    "<br>_________________________\n",
    "<br>Epochs:  6\n",
    "<br>Train on 15964 samples, validate on 3992 samples\n",
    "<br>15964/15964 [==============================] - 201s 13ms/sample - loss: 0.3339 - accuracy: 0.8566 - val_loss: 0.4832 - val_accuracy: 0.7851\n",
    "<br>4990/4990 - 16s - loss: 0.4743 - accuracy: 0.7928"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from sentdex \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from numpy import array \n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = array(pickle.load(open(\"Y.pickle\",\"rb\")))\n",
    "\n",
    "count = int(len(X) * 0.6)\n",
    "\n",
    "X_test = X[count:]\n",
    "X = X[:count]\n",
    "\n",
    "y_test = y[count:]\n",
    "y = y[:count]\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=3, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "#9979/9979 - 31s - loss: 47.4771 - accuracy: 0.6454\n",
    "#[47.47709955253108, 0.6453552]"
   ]
  }
 ]
}